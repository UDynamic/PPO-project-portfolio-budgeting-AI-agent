{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0502ac-d93b-4a4d-8f6a-c3a96ef3029a",
   "metadata": {},
   "source": [
    "# <div align=\"center\">The Single Project Budgetting Agent<div/>\n",
    "    \n",
    "<br/><br/>\n",
    "**as an introductory educational practice for the ultimate goal of developing and training the complex agent for optimizing the budgeting of the portfolio, I'll start by developing the single project agent**\n",
    "\n",
    "<br/><br>\n",
    "\n",
    "---\n",
    "\n",
    "## <div align=\"center\">Importing the modules<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3979fed5-6f8a-4ad5-8401-22cf80b4cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1: imports and basic config\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# For stable-baselines3 later\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cead8-6fb8-4275-a6e9-c3212e32948d",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">Designing environment<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7478e6-99e6-4918-8e85-3ec0cde437da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: the custom environment class (Gymnasium API)\n",
    "class SingleProjectBudgetEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Single Project Budget Allocation environment (Gymnasium API).\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(3)\n",
    "        [remaining_budget_frac, timesteps_left_frac, cumulative_spent_frac]\n",
    "        All in [0,1]\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete(4) -> map to fractions of *remaining budget*:\n",
    "            0 -> 0.0\n",
    "            1 -> 0.25\n",
    "            2 -> 0.5\n",
    "            3 -> 1.0\n",
    "\n",
    "    Rewards:\n",
    "        +1 for progress (allocated > 0 and not overspend)\n",
    "        -10 for overspend (defensive)\n",
    "        +100 for completion (cumulative_spent >= completion_threshold)\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self,\n",
    "                 total_budget: float = 100.0,\n",
    "                 max_timesteps: int = 10,\n",
    "                 completion_threshold: float = 90.0,\n",
    "                 render_mode: str | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.total_budget = float(total_budget)\n",
    "        self.max_timesteps = int(max_timesteps)\n",
    "        self.completion_threshold = float(completion_threshold)\n",
    "\n",
    "        # Action mapping (fractions of remaining budget)\n",
    "        self.action_map = [0.0, 0.25, 0.5, 1.0]\n",
    "\n",
    "        # Define action & observation spaces\n",
    "        self.action_space = spaces.Discrete(len(self.action_map))\n",
    "        # obs: remaining_frac, timesteps_left_frac, cumulative_spent_frac\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        # internal state\n",
    "        self.current_step = None\n",
    "        self.remaining_budget = None\n",
    "        self.cumulative_spent = None\n",
    "        self.done = None\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        # Gymnasium seeding pattern\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.remaining_budget = float(self.total_budget)\n",
    "        self.cumulative_spent = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            self.remaining_budget / self.total_budget,\n",
    "            (self.max_timesteps - self.current_step) / max(1, self.max_timesteps),\n",
    "            self.cumulative_spent / self.total_budget\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        frac = self.action_map[int(action)]\n",
    "        allocated = frac * self.remaining_budget  # allocation is fraction of *current remaining* budget\n",
    "\n",
    "        reward = 0.0\n",
    "        info = {}\n",
    "\n",
    "        # Overspend defensive check (shouldn't happen with our action map)\n",
    "        if allocated > self.remaining_budget + 1e-8:\n",
    "            # overspend\n",
    "            reward += -10.0\n",
    "            self.done = True\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            obs = self._get_obs()\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        # Apply allocation\n",
    "        self.remaining_budget -= allocated\n",
    "        self.cumulative_spent += allocated\n",
    "\n",
    "        # Progress reward: +1 for making progress this step (allocated > 0)\n",
    "        if allocated > 0:\n",
    "            reward += 1.0\n",
    "\n",
    "        # Completion check\n",
    "        if self.cumulative_spent >= self.completion_threshold:\n",
    "            reward += 100.0\n",
    "            self.done = True\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            obs = self._get_obs()\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        # Step advancement\n",
    "        self.current_step += 1\n",
    "        # If reached max timesteps -> truncated episode\n",
    "        if self.current_step >= self.max_timesteps:\n",
    "            self.done = True\n",
    "            terminated = False\n",
    "            truncated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            print(f\"Step {self.current_step}: remaining={self.remaining_budget:.2f}, \"\n",
    "                  f\"spent={self.cumulative_spent:.2f}\")\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf0bca-8270-4fc6-a2e0-663f471d3598",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">sanity check<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96adb62f-2404-4eb1-be7b-f9c5f718b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial obs: [1. 1. 0.]\n",
      "Step 1: remaining=75.00, spent=25.00\n",
      "reward 1.0 term False trunc False\n",
      "Step 2: remaining=56.25, spent=43.75\n",
      "reward 1.0 term False trunc False\n",
      "Step 3: remaining=42.19, spent=57.81\n",
      "reward 1.0 term False trunc False\n",
      "Step 4: remaining=31.64, spent=68.36\n",
      "reward 1.0 term False trunc False\n",
      "Step 5: remaining=23.73, spent=76.27\n",
      "reward 1.0 term False trunc False\n"
     ]
    }
   ],
   "source": [
    "# cell: sanity check\n",
    "env = SingleProjectBudgetEnv(total_budget=100, max_timesteps=10, completion_threshold=90.0, render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "print(\"Initial obs:\", obs)\n",
    "# try a greedy policy: always 25% of remaining\n",
    "for _ in range(5):\n",
    "    obs, r, term, trunc, info = env.step(1)  # action 1 -> 25%\n",
    "    env.render()\n",
    "    print(\"reward\", r, \"term\", term, \"trunc\", trunc)\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e949d0-abaa-4fd4-a162-5b958e82ca55",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">vectorized & normalized envs and model<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acafac82-0d10-4860-b661-2f090c9f7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: create vectorized & normalized envs and model\n",
    "log_dir = \"./logs_sb3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def make_env(rank=0):\n",
    "    def _init():\n",
    "        env = SingleProjectBudgetEnv(total_budget=100, max_timesteps=10, completion_threshold=90.0)\n",
    "        # Monitor writes episode info to csv for analysis if filename provided\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "n_envs = 4\n",
    "venv = DummyVecEnv([make_env(i) for i in range(n_envs)])\n",
    "# Normalize observations (not rewards here). Stable Baselines3 has VecNormalize.\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be73a9-aed9-4b34-bf11-6dff64ef2350",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">model<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffde900-a498-41b0-acac-2fe7a38a2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# cell: create model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=venv,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tb_logs/\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,     # amount of experience per environment per update\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2    # important PPO hyperparameter (explained below)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f99dd7-0b79-4d81-b93c-fabf82ad15a3",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">Training<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064ab05e-1f30-4f17-bb4a-23a23941295a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tb_logs/PPO_9\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.53     |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 1835     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.55        |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1121        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036266834 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00141    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.5e+03     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 5.27e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.89        |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 980         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045689955 |\n",
      "|    clip_fraction        | 0.807       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -0.000178   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 543         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.125      |\n",
      "|    value_loss           | 2.2e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.45        |\n",
      "|    ep_rew_mean          | 101         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058953993 |\n",
      "|    clip_fraction        | 0.933       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | -0.00016    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.146      |\n",
      "|    value_loss           | 593         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.07       |\n",
      "|    ep_rew_mean          | 101        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 892        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 45         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20121253 |\n",
      "|    clip_fraction        | 0.976      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.513     |\n",
      "|    explained_variance   | -7.26e-05  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0256     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.147     |\n",
      "|    value_loss           | 23.2       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 101       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 869       |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 56        |\n",
      "|    total_timesteps      | 49152     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6042706 |\n",
      "|    clip_fraction        | 0.108     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0169   |\n",
      "|    explained_variance   | -8.73e-05 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.038    |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0556   |\n",
      "|    value_loss           | 0.0375    |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01         |\n",
      "|    ep_rew_mean          | 101          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.684315e-05 |\n",
      "|    clip_fraction        | 0.00011      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.000898    |\n",
      "|    explained_variance   | -0.5         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.43e-06     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    value_loss           | 3.15e-06     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# cell: train\n",
    "total_timesteps = 50_000  # small for a toy env; increase to 200k+ for sturdier learning\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "# save model\n",
    "model.save(\"ppo_budget_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329f7f0-4c50-44af-bce1-892c8c8e5316",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">Visualization<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e408ffff-0039-409a-a715-e011f1e44f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-24d539c738efb43c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-24d539c738efb43c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the command for tensorboard: tensorboard --logdir ./tb_logs\n",
    "\n",
    "%load_ext tensorboard \n",
    "%tensorboard --logdir ./tb_logs/PPO_7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
