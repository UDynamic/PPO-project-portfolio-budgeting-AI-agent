{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3979fed5-6f8a-4ad5-8401-22cf80b4cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1: imports and basic config\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# For stable-baselines3 later\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7478e6-99e6-4918-8e85-3ec0cde437da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: the custom environment class (Gymnasium API)\n",
    "class SingleProjectBudgetEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Single Project Budget Allocation environment (Gymnasium API).\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(3)\n",
    "        [remaining_budget_frac, timesteps_left_frac, cumulative_spent_frac]\n",
    "        All in [0,1]\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete(4) -> map to fractions of *remaining budget*:\n",
    "            0 -> 0.0\n",
    "            1 -> 0.25\n",
    "            2 -> 0.5\n",
    "            3 -> 1.0\n",
    "\n",
    "    Rewards:\n",
    "        +1 for progress (allocated > 0 and not overspend)\n",
    "        -10 for overspend (defensive)\n",
    "        +100 for completion (cumulative_spent >= completion_threshold)\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self,\n",
    "                 total_budget: float = 100.0,\n",
    "                 max_timesteps: int = 10,\n",
    "                 completion_threshold: float = 90.0,\n",
    "                 render_mode: str | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.total_budget = float(total_budget)\n",
    "        self.max_timesteps = int(max_timesteps)\n",
    "        self.completion_threshold = float(completion_threshold)\n",
    "\n",
    "        # Action mapping (fractions of remaining budget)\n",
    "        self.action_map = [0.0, 0.25, 0.5, 1.0]\n",
    "\n",
    "        # Define action & observation spaces\n",
    "        self.action_space = spaces.Discrete(len(self.action_map))\n",
    "        # obs: remaining_frac, timesteps_left_frac, cumulative_spent_frac\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        # internal state\n",
    "        self.current_step = None\n",
    "        self.remaining_budget = None\n",
    "        self.cumulative_spent = None\n",
    "        self.done = None\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        # Gymnasium seeding pattern\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.remaining_budget = float(self.total_budget)\n",
    "        self.cumulative_spent = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            self.remaining_budget / self.total_budget,\n",
    "            (self.max_timesteps - self.current_step) / max(1, self.max_timesteps),\n",
    "            self.cumulative_spent / self.total_budget\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        frac = self.action_map[int(action)]\n",
    "        allocated = frac * self.remaining_budget  # allocation is fraction of *current remaining* budget\n",
    "\n",
    "        reward = 0.0\n",
    "        info = {}\n",
    "\n",
    "        # Overspend defensive check (shouldn't happen with our action map)\n",
    "        if allocated > self.remaining_budget + 1e-8:\n",
    "            # overspend\n",
    "            reward += -10.0\n",
    "            self.done = True\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            obs = self._get_obs()\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        # Apply allocation\n",
    "        self.remaining_budget -= allocated\n",
    "        self.cumulative_spent += allocated\n",
    "\n",
    "        # Progress reward: +1 for making progress this step (allocated > 0)\n",
    "        if allocated > 0:\n",
    "            reward += 1.0\n",
    "\n",
    "        # Completion check\n",
    "        if self.cumulative_spent >= self.completion_threshold:\n",
    "            reward += 100.0\n",
    "            self.done = True\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            obs = self._get_obs()\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        # Step advancement\n",
    "        self.current_step += 1\n",
    "        # If reached max timesteps -> truncated episode\n",
    "        if self.current_step >= self.max_timesteps:\n",
    "            self.done = True\n",
    "            terminated = False\n",
    "            truncated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            print(f\"Step {self.current_step}: remaining={self.remaining_budget:.2f}, \"\n",
    "                  f\"spent={self.cumulative_spent:.2f}\")\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adb62f-2404-4eb1-be7b-f9c5f718b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: sanity check\n",
    "env = SingleProjectBudgetEnv(total_budget=100, max_timesteps=10, completion_threshold=90.0, render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "print(\"Initial obs:\", obs)\n",
    "# try a greedy policy: always 25% of remaining\n",
    "for _ in range(5):\n",
    "    obs, r, term, trunc, info = env.step(1)  # action 1 -> 25%\n",
    "    env.render()\n",
    "    print(\"reward\", r, \"term\", term, \"trunc\", trunc)\n",
    "    if term or trunc:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafac82-0d10-4860-b661-2f090c9f7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: create vectorized & normalized envs and model\n",
    "log_dir = \"./logs_sb3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def make_env(rank=0):\n",
    "    def _init():\n",
    "        env = SingleProjectBudgetEnv(total_budget=100, max_timesteps=10, completion_threshold=90.0)\n",
    "        # Monitor writes episode info to csv for analysis if filename provided\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "n_envs = 4\n",
    "venv = DummyVecEnv([make_env(i) for i in range(n_envs)])\n",
    "# Normalize observations (not rewards here). Stable Baselines3 has VecNormalize.\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffde900-a498-41b0-acac-2fe7a38a2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: create model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=venv,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tb_logs/\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,     # amount of experience per environment per update\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2    # important PPO hyperparameter (explained below)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ab05e-1f30-4f17-bb4a-23a23941295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: train\n",
    "total_timesteps = 50_000  # small for a toy env; increase to 200k+ for sturdier learning\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "# save model\n",
    "model.save(\"ppo_budget_agent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
