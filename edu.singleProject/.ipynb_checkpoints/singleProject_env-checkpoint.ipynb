{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0502ac-d93b-4a4d-8f6a-c3a96ef3029a",
   "metadata": {},
   "source": [
    "# <div align=\"center\">The Single Project Budgetting Agent<div/>\n",
    "    \n",
    "<br/><br/>\n",
    "**as an introductory educational practice for the ultimate goal of developing and training the complex agent for optimizing the budgeting of the portfolio, I'll start by developing the single project agent**\n",
    "\n",
    "<br/><br>\n",
    "\n",
    "---\n",
    "\n",
    "## <div align=\"center\">Importing the modules<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3979fed5-6f8a-4ad5-8401-22cf80b4cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1: imports and basic config\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# For stable-baselines3 later\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677cead8-6fb8-4275-a6e9-c3212e32948d",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">Designing environment<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7478e6-99e6-4918-8e85-3ec0cde437da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: the custom environment class (Gymnasium API)\n",
    "class SingleProjectBudgetEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Single Project Budget Allocation environment (Gymnasium API).\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(3)\n",
    "        [remaining_budget_frac, timesteps_left_frac, cumulative_spent_frac]\n",
    "        All in [0,1]\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete(4) -> map to fractions of *remaining budget*:\n",
    "            0 -> 0.0\n",
    "            1 -> 0.25\n",
    "            2 -> 0.5\n",
    "            3 -> 1.0\n",
    "\n",
    "    Rewards:\n",
    "        +1 for progress (allocated > 0 and not overspend)\n",
    "        -10 for overspend (defensive)\n",
    "        +100 for completion (cumulative_spent >= completion_threshold)\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self,\n",
    "                 total_budget: float = 100.0,\n",
    "                 max_timesteps: int = 10,\n",
    "                 completion_threshold: float = 90.0,\n",
    "                 render_mode: str | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.total_budget = float(total_budget)\n",
    "        self.max_timesteps = int(max_timesteps)\n",
    "        self.completion_threshold = float(completion_threshold)\n",
    "\n",
    "        # Action mapping (fractions of remaining budget)\n",
    "        self.action_map = [0.0, 0.25, 0.5, 1.0]\n",
    "\n",
    "        # Define action & observation spaces\n",
    "        self.action_space = spaces.Discrete(len(self.action_map))\n",
    "        # obs: remaining_frac, timesteps_left_frac, cumulative_spent_frac\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        # internal state\n",
    "        self.current_step = None\n",
    "        self.remaining_budget = None\n",
    "        self.cumulative_spent = None\n",
    "        self.done = None\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        # Gymnasium seeding pattern\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.remaining_budget = float(self.total_budget)\n",
    "        self.cumulative_spent = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            self.remaining_budget / self.total_budget,\n",
    "            (self.max_timesteps - self.current_step) / max(1, self.max_timesteps),\n",
    "            self.cumulative_spent / self.total_budget\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "        frac = self.action_map[int(action)]\n",
    "        allocated = frac * self.remaining_budget  # allocation is fraction of *current remaining* budget\n",
    "\n",
    "        reward = 0.0\n",
    "        info = {}\n",
    "\n",
    "        # Overspend defensive check (shouldn't happen with our action map)\n",
    "        if allocated > self.remaining_budget + 1e-8:\n",
    "            # overspend\n",
    "            reward += -10.0\n",
    "            self.done = True\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            obs = self._get_obs()\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        # Apply allocation\n",
    "        self.remaining_budget -= allocated\n",
    "        self.cumulative_spent += allocated\n",
    "\n",
    "        # Progress reward: +1 for making progress this step (allocated > 0)\n",
    "        if allocated > 0:\n",
    "            reward += 1.0\n",
    "\n",
    "        # Completion check\n",
    "        if self.cumulative_spent >= self.completion_threshold:\n",
    "            reward += 100.0\n",
    "            self.done = True\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            obs = self._get_obs()\n",
    "            return obs, reward, terminated, truncated, info\n",
    "\n",
    "        # Step advancement\n",
    "        self.current_step += 1\n",
    "        # If reached max timesteps -> truncated episode\n",
    "        if self.current_step >= self.max_timesteps:\n",
    "            self.done = True\n",
    "            terminated = False\n",
    "            truncated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            print(f\"Step {self.current_step}: remaining={self.remaining_budget:.2f}, \"\n",
    "                  f\"spent={self.cumulative_spent:.2f}\")\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf0bca-8270-4fc6-a2e0-663f471d3598",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">sanity check<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96adb62f-2404-4eb1-be7b-f9c5f718b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial obs: [1. 1. 0.]\n",
      "Step 1: remaining=75.00, spent=25.00\n",
      "reward 1.0 term False trunc False\n",
      "Step 2: remaining=56.25, spent=43.75\n",
      "reward 1.0 term False trunc False\n",
      "Step 3: remaining=42.19, spent=57.81\n",
      "reward 1.0 term False trunc False\n",
      "Step 4: remaining=31.64, spent=68.36\n",
      "reward 1.0 term False trunc False\n",
      "Step 5: remaining=23.73, spent=76.27\n",
      "reward 1.0 term False trunc False\n"
     ]
    }
   ],
   "source": [
    "# cell: sanity check\n",
    "env = SingleProjectBudgetEnv(total_budget=100, max_timesteps=10, completion_threshold=90.0, render_mode=\"human\")\n",
    "obs, info = env.reset()\n",
    "print(\"Initial obs:\", obs)\n",
    "# try a greedy policy: always 25% of remaining\n",
    "for _ in range(5):\n",
    "    obs, r, term, trunc, info = env.step(1)  # action 1 -> 25%\n",
    "    env.render()\n",
    "    print(\"reward\", r, \"term\", term, \"trunc\", trunc)\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e949d0-abaa-4fd4-a162-5b958e82ca55",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">vectorized & normalized envs and model<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acafac82-0d10-4860-b661-2f090c9f7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell: create vectorized & normalized envs and model\n",
    "log_dir = \"./logs_sb3/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def make_env(rank=0):\n",
    "    def _init():\n",
    "        env = SingleProjectBudgetEnv(total_budget=100, max_timesteps=10, completion_threshold=90.0)\n",
    "        # Monitor writes episode info to csv for analysis if filename provided\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "n_envs = 4\n",
    "venv = DummyVecEnv([make_env(i) for i in range(n_envs)])\n",
    "# Normalize observations (not rewards here). Stable Baselines3 has VecNormalize.\n",
    "venv = VecNormalize(venv, norm_obs=True, norm_reward=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be73a9-aed9-4b34-bf11-6dff64ef2350",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">model<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffde900-a498-41b0-acac-2fe7a38a2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# cell: create model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=venv,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tb_logs/\",\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,     # amount of experience per environment per update\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2    # important PPO hyperparameter (explained below)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f99dd7-0b79-4d81-b93c-fabf82ad15a3",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">Training<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064ab05e-1f30-4f17-bb4a-23a23941295a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tb_logs/PPO_6\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.79     |\n",
      "|    ep_rew_mean     | 103      |\n",
      "| time/              |          |\n",
      "|    fps             | 2612     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.61        |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1196        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035417084 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00323    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.42e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 5.21e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.19        |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 964         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035188474 |\n",
      "|    clip_fraction        | 0.795       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.000391    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 548         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.112      |\n",
      "|    value_loss           | 2.16e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.37        |\n",
      "|    ep_rew_mean          | 101         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060952954 |\n",
      "|    clip_fraction        | 0.923       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.000251   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.146      |\n",
      "|    value_loss           | 575         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.25       |\n",
      "|    ep_rew_mean          | 101        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 834        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14609644 |\n",
      "|    clip_fraction        | 0.971      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | -0.000225  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.12       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.148     |\n",
      "|    value_loss           | 21.2       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 101        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 60         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78352094 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0264    |\n",
      "|    explained_variance   | -0.000106  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0431    |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0791    |\n",
      "|    value_loss           | 0.0856     |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 101           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 790           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 72            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010655995 |\n",
      "|    clip_fraction        | 0.000647      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00194      |\n",
      "|    explained_variance   | -62.6         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.46e-07      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000728     |\n",
      "|    value_loss           | 3.22e-05      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# cell: train\n",
    "total_timesteps = 50_000  # small for a toy env; increase to 200k+ for sturdier learning\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "# save model\n",
    "model.save(\"ppo_budget_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329f7f0-4c50-44af-bce1-892c8c8e5316",
   "metadata": {},
   "source": [
    "---\n",
    "## <div align=\"center\">Visualization<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86df0c-53ab-4424-9677-dc01432513e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
